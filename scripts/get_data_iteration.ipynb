{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5df6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_following_df_from_files(accounts_path):\n",
    "    \n",
    "    followings_df = []\n",
    "    \n",
    "    accounts = os.listdir(accounts_path)\n",
    "    \n",
    "    for account in accounts:\n",
    "        acc_path = os.path.join(accounts_path, account)\n",
    "        try:\n",
    "            df = pd.read_json(os.path.join(acc_path, 'following.json'))\n",
    "            followings_df.append(df)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            continue\n",
    "    return followings_df\n",
    "\n",
    "def load_trained_model(path: str):\n",
    "    loaded_model = pickle.load(open(path, 'rb'))\n",
    "    return loaded_model\n",
    "\n",
    "def clean_text(text: str):\n",
    "    text=re.sub(r'@[A-Za-z0-9]+','',text) ## removing @ mention\n",
    "    text=re.sub(r'#','',text)             ## removing # symbol\n",
    "    text=re.sub(r'RT[\\s]+','',text)  ## removing RT followed byspace\n",
    "    text=re.sub(r'https?:\\/\\/\\S+','',text) ## removing https\n",
    "    return text\n",
    "\n",
    "def filter_en_accounts(text: str):\n",
    "    try:\n",
    "        if text:\n",
    "            return detect(text) == 'en'\n",
    "        else:\n",
    "            return True\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "def filter_already_downloaded_accounts(acc_name: str, all_acc):\n",
    "    return acc_name not in all_acc\n",
    "            \n",
    "def get_tweets_for_all_accounts(api, anti_save_dir, pro_save_dir, neutral_save_dir, accounts):\n",
    "    \n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "    pretrained_vectorizer = load_trained_model(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\models\\vectorizer_1000_tweets_final_updated_after_1_iteration.h5')\n",
    "    pretrained_svm = load_trained_model(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\models\\svm_1000_tweets_final_updated_after_1_iteration.h5')\n",
    "    \n",
    "    \n",
    "    for acc in tqdm(accounts):\n",
    "        \n",
    "        save_path_anti = os.path.join(anti_save_dir, acc)\n",
    "        save_path_pro = os.path.join(pro_save_dir, acc)\n",
    "        save_path_neutral = os.path.join(neutral_save_dir, acc)\n",
    "        \n",
    "        if os.path.exists(os.path.join(save_path_anti, 'tweets.json')) or os.path.exists(os.path.join(save_path_pro, 'tweets.json')) \\\n",
    "        or os.path.exists(os.path.join(save_path_neutral, 'tweets.json')):\n",
    "            print(f'Account: {acc} skipped - already downloaded')\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            df = get_account_tweets(api, acc)\n",
    "            df['filtered_text'] = df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in english_stopwords]))\n",
    "            df_acc = df.groupby(['account'], as_index = False).agg({'filtered_text': ' '.join, 'language': pd.Series.mode})\n",
    "            if df_acc['language'][0] != 'en':\n",
    "                print(f'Account: {acc} skipped as not english language')\n",
    "                continue\n",
    "            tweets_vectorized = pretrained_vectorizer.transform([df_acc['filtered_text'][0]])\n",
    "            predicted_class = pretrained_svm.predict(tweets_vectorized)[0]\n",
    "\n",
    "            # anti hubs\n",
    "            if predicted_class == 0:\n",
    "                print(f'Account: {acc} was clasified as anti hub')\n",
    "                save_path = os.path.join(anti_save_dir, acc)\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.mkdir(save_path)\n",
    "                    df.to_json(os.path.join(save_path, 'tweets.json'))\n",
    "                    \n",
    "            if predicted_class == 1:\n",
    "                print(f'Account: {acc} was clasified as neutral hub')\n",
    "                save_path = os.path.join(neutral_save_dir, acc)\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.mkdir(save_path)\n",
    "                    df.to_json(os.path.join(save_path, 'tweets.json'))\n",
    "\n",
    "            # pro hubs      \n",
    "            elif predicted_class == 2:\n",
    "                print(f'Account: {acc} was clasified as pro hub')\n",
    "                save_path = os.path.join(pro_save_dir, acc)\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.mkdir(save_path)\n",
    "                    df.to_json(os.path.join(save_path, 'tweets.json'))\n",
    "                    \n",
    "                    \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            continue\n",
    "            \n",
    "\n",
    "def get_account_tweets(api, acc_name: str, max_tweets=1000):\n",
    "    \n",
    "    try:\n",
    "        all_tweets = []     \n",
    "        tweets = api.user_timeline(screen_name=acc_name, count=200, include_rts=True, tweet_mode='extended')\n",
    "        if tweets:\n",
    "            all_tweets.extend(tweets)\n",
    "            oldest_id = tweets[-1].id\n",
    "\n",
    "            while len(tweets) > 0 and len(all_tweets) <= max_tweets:\n",
    "                tweets = api.user_timeline(screen_name=acc_name, count=200, include_rts=True, max_id=oldest_id-1, tweet_mode='extended')\n",
    "                all_tweets.extend(tweets)\n",
    "                oldest_id = all_tweets[-1].id\n",
    "\n",
    "        final_tweets = [[tweet.id_str, tweet.created_at, tweet.favorite_count, tweet.retweet_count, tweet.lang, tweet.full_text.encode(\"utf-8\").decode(\"utf-8\")] \n",
    "                 for idx, tweet in enumerate(all_tweets)]\n",
    "\n",
    "        print(f'Downloaded {len(final_tweets)} tweets for account {acc_name}')\n",
    "        df = pd.DataFrame(final_tweets,columns=[\"id\",\"created_at\",\"favorite_count\",\"retweet_count\", \"language\", \"text\"])\n",
    "        df['clean_text']=df['text'].apply(clean_text)\n",
    "        df['account'] = acc_name\n",
    "        return df\n",
    "    \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    \n",
    "def itemgetter(*items):\n",
    "    if len(items) == 1:\n",
    "        item = items[0]\n",
    "        def g(obj):\n",
    "            return obj[item]\n",
    "    else:\n",
    "        def g(obj):\n",
    "            return tuple(obj[item] for item in items)\n",
    "    return g\n",
    "\n",
    "def get_following_ids(api, screen_name):\n",
    "    print('Getting Following ids of', screen_name)\n",
    "    following_ids = []\n",
    "    try:\n",
    "        users_id = tweepy.Cursor(api.friends_ids, screen_name = screen_name, wait_on_rate_limit = True, count = 5000, lang=['en'])\n",
    "        \n",
    "        for user_id in users_id.items():\n",
    "            following_ids.append(user_id)\n",
    "            \n",
    "    except tweepy.TweepError as e:\n",
    "        print(e)\n",
    "        \n",
    "    print(f'Fetched number of following ids for {screen_name} : {len(following_ids)}')\n",
    "    return following_ids\n",
    "\n",
    "def get_followers_ids(screen_name, max_number):\n",
    "    print('Getting Followers ids of', screen_name)\n",
    "    followers_ids = []\n",
    "    try:\n",
    "        while len(followers_ids) <= max_number:\n",
    "            users_id = tweepy.Cursor(api.followers_ids, screen_name = screen_name, wait_on_rate_limit = True, count = 5000, lang=['en'])\n",
    "            for user_id in users_id.items():\n",
    "                followers_ids.append(user_id)\n",
    "                \n",
    "    except tweepy.TweepError as e:\n",
    "            print('Going to sleep: ', e)\n",
    "            time.sleep(60)\n",
    "            \n",
    "    print(f'Fetched number of followers ids for {screen_name} : {len(followers_ids)}')\n",
    "    return followers_ids\n",
    "\n",
    "\n",
    "def get_following(screen_name):\n",
    "    print('Getting Following list of ', screen_name)\n",
    "    following = []\n",
    "    users = tweepy.Cursor(api.friends, screen_name = screen_name, wait_on_rate_limit = True, count=5000, lang=['en'])\n",
    "    for user in users.items():\n",
    "        try:\n",
    "            following.append(user)\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Going to sleep:\", e)\n",
    "            time.sleep(60)\n",
    "    print(f'Fetched number of followings for {screen_name} : {len(following)}')          \n",
    "    return following\n",
    "\n",
    "\n",
    "def get_data(api, save_dir, accounts):\n",
    "    \n",
    "    for acc in tqdm(accounts):\n",
    "\n",
    "        save_path = os.path.join(save_dir, acc)\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "            \n",
    "        # ACCOUNT DATA\n",
    "        try:\n",
    "            \n",
    "            if os.path.exists(os.path.join(save_path, 'account.json')):\n",
    "                print(f'Account: {acc} skipped - already downloaded')\n",
    "                continue\n",
    "            \n",
    "            acc_data = api.get_user(screen_name = acc, wait_on_rate_limit = True)\n",
    "            ob = {\n",
    "                    'ID':acc_data.id,\n",
    "                    'Screen_Name':acc_data.screen_name,\n",
    "                    'Description': acc_data.description,\n",
    "                    'StatusesCount':acc_data.statuses_count,\n",
    "                    'Follower_Count':acc_data.followers_count,\n",
    "                    'Following_Count':acc_data.friends_count\n",
    "                    }\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(save_path, 'account.json'), 'w') as f:\n",
    "            json.dump(ob, f)\n",
    "            \n",
    "        if ob['Following_Count'] > 20000:\n",
    "            print(f\"Skipped {acc} because too many followings - {ob['Following_Count']}\")\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(os.path.join(save_path, 'following.json')):\n",
    "            print(f'Account: {acc} skipped - already downloaded')\n",
    "            continue\n",
    "        \n",
    "        # FOLLOWING DATA\n",
    "        data = []\n",
    "        following_ids = get_following_ids(api, acc)\n",
    "        for i in range(0, len(following_ids), 100):\n",
    "            try:\n",
    "                chunk = following_ids[i:i+100]\n",
    "                users_chunk = api.lookup_users(user_ids=chunk)\n",
    "                for user in users_chunk:\n",
    "                    try:\n",
    "                        ob = {\n",
    "                        'ID':user.id,\n",
    "                        'Screen_Name':user.screen_name,\n",
    "                        'Description': user.description,\n",
    "                        'StatusesCount':user.statuses_count,\n",
    "                        'Follower_Count':user.followers_count,\n",
    "                        'Following_Count':user.friends_count\n",
    "                    } \n",
    "                        data.append(ob)\n",
    "                    except Exception as ex:\n",
    "                        print(ex)\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Something went wrong, skipping...')\n",
    "                continue\n",
    "                \n",
    "        with open(os.path.join(save_path, 'following.json'), 'w') as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_anti_hubs_followings_df = get_following_df_from_files(anti_accounts_path)\n",
    "hand_picked_anti_hubs_followings_df = get_following_df_from_files(hand_picked_new_anti_hubs_path)\n",
    "\n",
    "base_pro_hubs_followings_df = get_following_df_from_files(pro_accounts_path)\n",
    "hand_picked_pro_hubs_followings_df = get_following_df_from_files(hand_picked_new_pro_hubs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings = base_anti_hubs_followings_df + hand_picked_anti_hubs_followings_df + base_pro_hubs_followings_df + hand_picked_pro_hubs_followings_df\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bec656",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "# filter only for hubs\n",
    "only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "# clean description for language detectio\n",
    "only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# # take only en accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "# # remove already downloaded accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d4bbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "only_hubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5418ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f33656",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_save_dir_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_1'\n",
    "pro_save_dir_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f399579",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumerKey = 'qFXNatPD5C2JFEQPlMTXFUr8x'\n",
    "consumerSecret = 'VCTI9keIp1mqIKLwpuoApI7HSe5b0SpeUHvcQ676J3SOjuuISM'\n",
    "accessToken = '1371497692069789697-0tx6gputswEwOMlwGfUy4VKBID5SCg'\n",
    "accessTokenSecret = 'zSFpItEQObd4PBc7E0PFAvoJApglHJqbcT37TW928ji5P'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=False, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff560b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_for_all_accounts(anti_save_dir_path, pro_save_dir_path, new_hubs_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b293b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW DOWNLOAD ALL FOLLOWINGS FOR NEW PRO AND ANTI HUBS AND START AGAIN ITERATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anti_accounts = os.listdir(anti_save_dir_path)\n",
    "get_data(anti_save_dir_path, new_anti_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f57ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pro_accounts = os.listdir(pro_save_dir_path)\n",
    "get_data(pro_save_dir_path, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b01e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accounts += new_pro_accounts\n",
    "all_accounts += new_anti_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6dec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f9f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5125a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34126f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "# filter only for hubs\n",
    "only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "# clean description for language detectio\n",
    "only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# # take only en accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "# # remove already downloaded accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_hubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c78840",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_save_dir_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_2'\n",
    "pro_save_dir_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_2'\n",
    "neutral_save_dir_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\all_neutral_accounts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumerKey = 'qFXNatPD5C2JFEQPlMTXFUr8x'\n",
    "consumerSecret = 'VCTI9keIp1mqIKLwpuoApI7HSe5b0SpeUHvcQ676J3SOjuuISM'\n",
    "accessToken = '1371497692069789697-0tx6gputswEwOMlwGfUy4VKBID5SCg'\n",
    "accessTokenSecret = 'zSFpItEQObd4PBc7E0PFAvoJApglHJqbcT37TW928ji5P'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=False, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_for_all_accounts(anti_save_dir_path, pro_save_dir_path, neutral_save_dir_path, new_hubs_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anti_accounts = os.listdir(anti_save_dir_path)\n",
    "get_data(anti_save_dir_path, new_anti_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pro_accounts = os.listdir(pro_save_dir_path)\n",
    "get_data(pro_save_dir_path, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd879b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_neutral_accounts = os.listdir(neutral_save_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accounts += new_pro_accounts\n",
    "all_accounts += new_anti_accounts\n",
    "all_accounts += new_neutral_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c3ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc31070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "# filter only for hubs\n",
    "only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "# clean description for language detectio\n",
    "only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# # take only en accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "# # remove already downloaded accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170593ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_hubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d072e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae4355",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_save_dir_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_3'\n",
    "pro_save_dir_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_3'\n",
    "neutral_save_dir_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\all_neutral_accounts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_for_all_accounts(anti_save_dir_path, pro_save_dir_path, neutral_save_dir_path, new_hubs_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24474d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anti_accounts = os.listdir(anti_save_dir_path)\n",
    "get_data(anti_save_dir_path, new_anti_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pro_accounts = os.listdir(pro_save_dir_path)\n",
    "get_data(pro_save_dir_path, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all downloaded accounts\n",
    "\n",
    "# ANTI = BASE HUBS + HAND PICKED HUBS\n",
    "anti_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\anti_scientific_data_2'\n",
    "hand_picked_new_anti_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_anti_hubs'\n",
    "\n",
    "# PRO = BASE HUBS + HAND PICKED HUBS\n",
    "pro_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\pro_scientific_data_2'\n",
    "hand_picked_new_pro_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_pro_hubs'\n",
    "\n",
    "with open(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\files\\neutral_accounts.txt', 'r') as f:\n",
    "    base_neutral_accounts = f.read().split(',')\n",
    "new_neutral_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\all_neutral_accounts'\n",
    "\n",
    "anti_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_1'\n",
    "pro_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_1'\n",
    "\n",
    "anti_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_2'\n",
    "pro_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_2'\n",
    "\n",
    "anti_save_dir_path_3 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_3'\n",
    "pro_save_dir_path_3= r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_3'\n",
    "\n",
    "\n",
    "\n",
    "# all base accounts which are already downloaded\n",
    "\n",
    "all_accounts = os.listdir(anti_accounts_path) + os.listdir(hand_picked_new_anti_hubs_path) + os.listdir(pro_accounts_path) \\\n",
    "+ os.listdir(hand_picked_new_pro_hubs_path) + os.listdir(new_neutral_accounts_path) + base_neutral_accounts \\\n",
    "+ os.listdir(anti_save_dir_path_1) + os.listdir(pro_save_dir_path_1) + os.listdir(anti_save_dir_path_2) \\\n",
    "+ os.listdir(pro_save_dir_path_2) + os.listdir(anti_save_dir_path_3) + os.listdir(pro_save_dir_path_3)\n",
    "\n",
    "all_accounts = [re.sub(r'@','', acc) for acc in all_accounts]\n",
    "\n",
    "print(len(all_accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ef144",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path_3)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689278b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a47e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "# filter only for hubs\n",
    "only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "# clean description for language detectio\n",
    "only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# # take only en accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "# # remove already downloaded accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fe26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_hubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df495735",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_4'\n",
    "pro_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c058df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_for_all_accounts(anti_save_dir_path_4, pro_save_dir_path_4, neutral_save_dir_path, new_hubs_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anti_accounts = os.listdir(anti_save_dir_path_4)\n",
    "get_data(anti_save_dir_path_4, new_anti_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d852ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pro_accounts = os.listdir(pro_save_dir_path_4)\n",
    "get_data(pro_save_dir_path_4, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all downloaded accounts\n",
    "\n",
    "# ANTI = BASE HUBS + HAND PICKED HUBS\n",
    "anti_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\anti_scientific_data_2'\n",
    "hand_picked_new_anti_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_anti_hubs'\n",
    "\n",
    "# PRO = BASE HUBS + HAND PICKED HUBS\n",
    "pro_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\pro_scientific_data_2'\n",
    "hand_picked_new_pro_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_pro_hubs'\n",
    "\n",
    "with open(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\files\\neutral_accounts.txt', 'r') as f:\n",
    "    base_neutral_accounts = f.read().split(',')\n",
    "new_neutral_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\all_neutral_accounts'\n",
    "\n",
    "anti_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_1'\n",
    "pro_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_1'\n",
    "\n",
    "anti_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_2'\n",
    "pro_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_2'\n",
    "\n",
    "anti_save_dir_path_3 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_3'\n",
    "pro_save_dir_path_3= r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_3'\n",
    "\n",
    "anti_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_4'\n",
    "pro_save_dir_path_4= r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_4'\n",
    "\n",
    "\n",
    "# all base accounts which are already downloaded\n",
    "\n",
    "all_accounts = os.listdir(anti_accounts_path) + os.listdir(hand_picked_new_anti_hubs_path) + os.listdir(pro_accounts_path) \\\n",
    "+ os.listdir(hand_picked_new_pro_hubs_path) + os.listdir(new_neutral_accounts_path) + base_neutral_accounts \\\n",
    "+ os.listdir(anti_save_dir_path_1) + os.listdir(pro_save_dir_path_1) + os.listdir(anti_save_dir_path_2) \\\n",
    "+ os.listdir(pro_save_dir_path_2) + os.listdir(anti_save_dir_path_3) + os.listdir(pro_save_dir_path_3) \\\n",
    "+ os.listdir(anti_save_dir_path_4) + os.listdir(pro_save_dir_path_4)\n",
    "\n",
    "all_accounts = [re.sub(r'@','', acc) for acc in all_accounts]\n",
    "\n",
    "print(len(all_accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5759dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path_4)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689692f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb33f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "# filter only for hubs\n",
    "only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "# clean description for language detectio\n",
    "only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# # take only en accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "# # remove already downloaded accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts, all_acc=all_accounts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_hubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_save_dir_path_5 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_5'\n",
    "pro_save_dir_path_5 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00489cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_for_all_accounts(anti_save_dir_path_5, pro_save_dir_path_5, neutral_save_dir_path, new_hubs_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anti_accounts = os.listdir(anti_save_dir_path_5)\n",
    "get_data(anti_save_dir_path_5, new_anti_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pro_accounts = os.listdir(pro_save_dir_path_5)\n",
    "get_data(pro_save_dir_path_5, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all downloaded accounts\n",
    "\n",
    "# ANTI = BASE HUBS + HAND PICKED HUBS\n",
    "anti_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\anti_scientific_data_2'\n",
    "hand_picked_new_anti_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_anti_hubs'\n",
    "\n",
    "# PRO = BASE HUBS + HAND PICKED HUBS\n",
    "pro_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\pro_scientific_data_2'\n",
    "hand_picked_new_pro_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_pro_hubs'\n",
    "\n",
    "with open(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\files\\neutral_accounts.txt', 'r') as f:\n",
    "    base_neutral_accounts = f.read().split(',')\n",
    "new_neutral_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\all_neutral_accounts'\n",
    "\n",
    "anti_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_1'\n",
    "pro_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_1'\n",
    "\n",
    "anti_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_2'\n",
    "pro_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_2'\n",
    "\n",
    "anti_save_dir_path_3 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_3'\n",
    "pro_save_dir_path_3 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_3'\n",
    "\n",
    "anti_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_4'\n",
    "pro_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_4'\n",
    "\n",
    "anti_save_dir_path_5 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_5'\n",
    "pro_save_dir_path_5 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_5'\n",
    "\n",
    "\n",
    "# all base accounts which are already downloaded\n",
    "\n",
    "all_accounts = os.listdir(anti_accounts_path) + os.listdir(hand_picked_new_anti_hubs_path) + os.listdir(pro_accounts_path) \\\n",
    "+ os.listdir(hand_picked_new_pro_hubs_path) + os.listdir(new_neutral_accounts_path) + base_neutral_accounts \\\n",
    "+ os.listdir(anti_save_dir_path_1) + os.listdir(pro_save_dir_path_1) + os.listdir(anti_save_dir_path_2) \\\n",
    "+ os.listdir(pro_save_dir_path_2) + os.listdir(anti_save_dir_path_3) + os.listdir(pro_save_dir_path_3) \\\n",
    "+ os.listdir(anti_save_dir_path_4) + os.listdir(pro_save_dir_path_4) \\\n",
    "+ os.listdir(anti_save_dir_path_5) + os.listdir(pro_save_dir_path_5)\n",
    "\n",
    "all_accounts = [re.sub(r'@','', acc) for acc in all_accounts]\n",
    "\n",
    "print(len(all_accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf126f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path_5)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ddcd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "# filter only for hubs\n",
    "only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "# clean description for language detectio\n",
    "only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# # take only en accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "# # remove already downloaded accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts, all_acc=all_accounts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_hubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e291f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0057cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_save_dir_path_6 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_6'\n",
    "pro_save_dir_path_6 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f883a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_for_all_accounts(anti_save_dir_path_6, pro_save_dir_path_6, neutral_save_dir_path, new_hubs_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced56626",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anti_accounts = os.listdir(anti_save_dir_path_6)\n",
    "get_data(anti_save_dir_path_6, new_anti_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pro_accounts = os.listdir(pro_save_dir_path_6)\n",
    "get_data(pro_save_dir_path_6, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all downloaded accounts\n",
    "\n",
    "# ANTI = BASE HUBS + HAND PICKED HUBS\n",
    "anti_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\anti_scientific_data_2'\n",
    "hand_picked_new_anti_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_anti_hubs'\n",
    "\n",
    "# PRO = BASE HUBS + HAND PICKED HUBS\n",
    "pro_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\pro_scientific_data_2'\n",
    "hand_picked_new_pro_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_pro_hubs'\n",
    "\n",
    "with open(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\files\\neutral_accounts.txt', 'r') as f:\n",
    "    base_neutral_accounts = f.read().split(',')\n",
    "new_neutral_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\all_neutral_accounts'\n",
    "\n",
    "anti_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_1'\n",
    "pro_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_1'\n",
    "\n",
    "anti_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_2'\n",
    "pro_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_2'\n",
    "\n",
    "anti_save_dir_path_3 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_3'\n",
    "pro_save_dir_path_3 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_3'\n",
    "\n",
    "anti_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_4'\n",
    "pro_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_4'\n",
    "\n",
    "anti_save_dir_path_5 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_5'\n",
    "pro_save_dir_path_5 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_5'\n",
    "\n",
    "anti_save_dir_path_6 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_6'\n",
    "pro_save_dir_path_6 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_6'\n",
    "\n",
    "\n",
    "# all base accounts which are already downloaded\n",
    "\n",
    "all_accounts = os.listdir(anti_accounts_path) + os.listdir(hand_picked_new_anti_hubs_path) + os.listdir(pro_accounts_path) \\\n",
    "+ os.listdir(hand_picked_new_pro_hubs_path) + os.listdir(new_neutral_accounts_path) + base_neutral_accounts \\\n",
    "+ os.listdir(anti_save_dir_path_1) + os.listdir(pro_save_dir_path_1) + os.listdir(anti_save_dir_path_2) \\\n",
    "+ os.listdir(pro_save_dir_path_2) + os.listdir(anti_save_dir_path_3) + os.listdir(pro_save_dir_path_3) \\\n",
    "+ os.listdir(anti_save_dir_path_4) + os.listdir(pro_save_dir_path_4) \\\n",
    "+ os.listdir(anti_save_dir_path_5) + os.listdir(pro_save_dir_path_5) \\\n",
    "+ os.listdir(anti_save_dir_path_6) + os.listdir(pro_save_dir_path_6)\n",
    "\n",
    "all_accounts = [re.sub(r'@','', acc) for acc in all_accounts]\n",
    "\n",
    "print(len(all_accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f56673",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path_6)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "# filter only for hubs\n",
    "only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "# clean description for language detectio\n",
    "only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# # take only en accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "# # remove already downloaded accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts, all_acc=all_accounts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc389aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_hubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb319779",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d34cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_save_dir_path_7 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_7'\n",
    "pro_save_dir_path_7 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b04e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_for_all_accounts(anti_save_dir_path_7, pro_save_dir_path_7, neutral_save_dir_path, new_hubs_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f017e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anti_accounts = os.listdir(anti_save_dir_path_7)\n",
    "get_data(anti_save_dir_path_7, new_anti_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pro_accounts = os.listdir(pro_save_dir_path_7)\n",
    "get_data(pro_save_dir_path_7, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3378430",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all downloaded accounts\n",
    "\n",
    "# ANTI = BASE HUBS + HAND PICKED HUBS\n",
    "anti_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\anti_scientific_data_2'\n",
    "hand_picked_new_anti_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_anti_hubs'\n",
    "\n",
    "# PRO = BASE HUBS + HAND PICKED HUBS\n",
    "pro_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\pro_scientific_data_2'\n",
    "hand_picked_new_pro_hubs_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\hand_picked_new_pro_hubs'\n",
    "\n",
    "with open(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\files\\neutral_accounts.txt', 'r') as f:\n",
    "    base_neutral_accounts = f.read().split(',')\n",
    "new_neutral_accounts_path = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\all_neutral_accounts'\n",
    "\n",
    "anti_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_1'\n",
    "pro_save_dir_path_1 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_1'\n",
    "\n",
    "anti_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_2'\n",
    "pro_save_dir_path_2 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_2'\n",
    "\n",
    "anti_save_dir_path_3 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_3'\n",
    "pro_save_dir_path_3 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_3'\n",
    "\n",
    "anti_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_4'\n",
    "pro_save_dir_path_4 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_4'\n",
    "\n",
    "anti_save_dir_path_5 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_5'\n",
    "pro_save_dir_path_5 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_5'\n",
    "\n",
    "anti_save_dir_path_6 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_6'\n",
    "pro_save_dir_path_6 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_6'\n",
    "\n",
    "anti_save_dir_path_7 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_7'\n",
    "pro_save_dir_path_7 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_7'\n",
    "\n",
    "\n",
    "# all base accounts which are already downloaded\n",
    "\n",
    "all_accounts = os.listdir(anti_accounts_path) + os.listdir(hand_picked_new_anti_hubs_path) + os.listdir(pro_accounts_path) \\\n",
    "+ os.listdir(hand_picked_new_pro_hubs_path) + os.listdir(new_neutral_accounts_path) + base_neutral_accounts \\\n",
    "+ os.listdir(anti_save_dir_path_1) + os.listdir(pro_save_dir_path_1) + os.listdir(anti_save_dir_path_2) \\\n",
    "+ os.listdir(pro_save_dir_path_2) + os.listdir(anti_save_dir_path_3) + os.listdir(pro_save_dir_path_3) \\\n",
    "+ os.listdir(anti_save_dir_path_4) + os.listdir(pro_save_dir_path_4) \\\n",
    "+ os.listdir(anti_save_dir_path_5) + os.listdir(pro_save_dir_path_5) \\\n",
    "+ os.listdir(anti_save_dir_path_6) + os.listdir(pro_save_dir_path_6) \\\n",
    "+ os.listdir(anti_save_dir_path_7) + os.listdir(pro_save_dir_path_7)\n",
    "\n",
    "all_accounts = [re.sub(r'@','', acc) for acc in all_accounts]\n",
    "\n",
    "print(len(all_accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path_7)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f587b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "# filter only for hubs\n",
    "only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "# clean description for language detectio\n",
    "only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# # take only en accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "# # remove already downloaded accounts\n",
    "only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts, all_acc=all_accounts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_hubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_save_dir_path_8 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_anti_hubs_auto_8'\n",
    "pro_save_dir_path_8 = r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\new_pro_hubs_auto_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b63a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_for_all_accounts(anti_save_dir_path_8, pro_save_dir_path_8, neutral_save_dir_path, new_hubs_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ae5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_anti_accounts = os.listdir(anti_save_dir_path_8)\n",
    "get_data(anti_save_dir_path_8, new_anti_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pro_accounts = os.listdir(pro_save_dir_path_8)\n",
    "get_data(pro_save_dir_path_8, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98082b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path_7)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d5819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all downloaded accounts\n",
    "\n",
    "# ANTI = BASE HUBS + HAND PICKED HUBS\n",
    "base_anti_accounts_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\base_anti_hubs'\n",
    "hand_picked_new_anti_hubs_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\hand_picked_base_anti_hubs'\n",
    "\n",
    "# PRO = BASE HUBS + HAND PICKED HUBS\n",
    "base_pro_accounts_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\base_pro_hubs'\n",
    "hand_picked_new_pro_hubs_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\hand_picked_base_pro_hubs'\n",
    "\n",
    "\n",
    "anti_save_dir_path_1 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_anti_hubs_1'\n",
    "pro_save_dir_path_1 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_pro_hubs_1'\n",
    "\n",
    "anti_save_dir_path_2 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_anti_hubs_2'\n",
    "pro_save_dir_path_2 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_pro_hubs_2'\n",
    "\n",
    "anti_save_dir_path_3 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_anti_hubs_3'\n",
    "pro_save_dir_path_3 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_pro_hubs_3'\n",
    "\n",
    "anti_save_dir_path_4 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_anti_hubs_4'\n",
    "pro_save_dir_path_4 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_pro_hubs_4'\n",
    "\n",
    "anti_save_dir_path_5 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_anti_hubs_5'\n",
    "pro_save_dir_path_5 = r'E:\\Twitter_data\\AUTOMATED_ITERATION_2\\new_pro_hubs_5'\n",
    "\n",
    "\n",
    "# all base accounts which are already downloaded\n",
    "\n",
    "all_accounts = os.listdir(anti_accounts_path) + os.listdir(hand_picked_new_anti_hubs_path) + os.listdir(pro_accounts_path) \\\n",
    "+ os.listdir(hand_picked_new_pro_hubs_path)  + os.listdir(anti_save_dir_path_1) + os.listdir(pro_save_dir_path_1) \\\n",
    "+ os.listdir(anti_save_dir_path_2) + os.listdir(pro_save_dir_path_2) + os.listdir(anti_save_dir_path_3) \\\n",
    "+ os.listdir(pro_save_dir_path_3) + os.listdir(anti_save_dir_path_4) + os.listdir(pro_save_dir_path_4) \\\n",
    "+ os.listdir(anti_save_dir_path_5) + os.listdir(pro_save_dir_path_5)\n",
    "\n",
    "all_accounts = [re.sub(r'@','', acc) for acc in all_accounts]\n",
    "\n",
    "print(len(all_accounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e47ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_anti_hubs_followings_df = get_following_df_from_files(base_anti_accounts_path)\n",
    "hand_picked_anti_hubs_followings_df = get_following_df_from_files(hand_picked_new_anti_hubs_path)\n",
    "\n",
    "base_pro_hubs_followings_df = get_following_df_from_files(base_pro_accounts_path)\n",
    "hand_picked_pro_hubs_followings_df = get_following_df_from_files(hand_picked_new_pro_hubs_path)\n",
    "\n",
    "anti_hubs_followings_df = get_following_df_from_files(anti_save_dir_path_2)\n",
    "pro_hubs_followings_df = get_following_df_from_files(pro_save_dir_path_2)\n",
    "\n",
    "\n",
    "all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "# drop accounts which have more than 5000 followings\n",
    "all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "all_followings_df = pd.concat(all_followings, ignore_index=True)\n",
    "all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "print(all_followings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70372bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f8c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad9634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE ACCOUNTS      \n",
    "# ANTI = BASE HUBS + HAND PICKED HUBS\n",
    "base_anti_accounts_path = r'E:\\Twitter_data\\BASE_HUBS\\anti_scientific_data_2'\n",
    "hand_picked_new_anti_hubs_path = r'E:\\Twitter_data\\BASE_HUBS\\hand_picked_new_anti_hubs'\n",
    "\n",
    "# PRO = BASE HUBS + HAND PICKED HUBS\n",
    "base_pro_accounts_path = r'E:\\Twitter_data\\BASE_HUBS\\pro_scientific_data_2'\n",
    "hand_picked_new_pro_hubs_path = r'E:\\Twitter_data\\BASE_HUBS\\hand_picked_new_pro_hubs'\n",
    "\n",
    "with open(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\files\\neutral_accounts.txt', 'r') as f:\n",
    "    base_neutral_accounts = f.read().split(',')\n",
    "new_neutral_accounts_path = r'E:\\Twitter_data\\BASE_HUBS\\neutral_data_2'\n",
    "\n",
    "all_base_accounts = os.listdir(base_anti_accounts_path) + os.listdir(hand_picked_new_anti_hubs_path) \\\n",
    "+ os.listdir(base_pro_accounts_path) + os.listdir(hand_picked_new_pro_hubs_path) + os.listdir(new_neutral_accounts_path) \\\n",
    "+ base_neutral_accounts\n",
    "all_base_accounts = [re.sub(r'@','', acc) for acc in all_base_accounts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b34cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_data_iteration():\n",
    "    \n",
    "    consumerKey = 'qFXNatPD5C2JFEQPlMTXFUr8x'\n",
    "    consumerSecret = 'VCTI9keIp1mqIKLwpuoApI7HSe5b0SpeUHvcQ676J3SOjuuISM'\n",
    "    accessToken = '1371497692069789697-0tx6gputswEwOMlwGfUy4VKBID5SCg'\n",
    "    accessTokenSecret = 'zSFpItEQObd4PBc7E0PFAvoJApglHJqbcT37TW928ji5P'\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "    auth.set_access_token(accessToken, accessTokenSecret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=False, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    \n",
    "    # BASE ACCOUNTS      \n",
    "    # ANTI = BASE HUBS + HAND PICKED HUBS\n",
    "    base_anti_accounts_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\base_anti_hubs'\n",
    "    hand_picked_new_anti_hubs_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\hand_picked_base_anti_hubs'\n",
    "\n",
    "    # PRO = BASE HUBS + HAND PICKED HUBS\n",
    "    base_pro_accounts_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\base_pro_hubs'\n",
    "    hand_picked_new_pro_hubs_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\hand_picked_base_pro_hubs'\n",
    "\n",
    "    with open(r'C:\\Users\\psrub\\Documents\\Python\\Twitter\\files\\neutral_accounts.txt', 'r') as f:\n",
    "        base_neutral_accounts = f.read().split(',')\n",
    "    new_neutral_accounts_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\all_neutral_accounts'\n",
    "    \n",
    "    all_base_accounts = os.listdir(base_anti_accounts_path) + os.listdir(hand_picked_new_anti_hubs_path) \\\n",
    "    + os.listdir(base_pro_accounts_path) + os.listdir(hand_picked_new_pro_hubs_path) + os.listdir(new_neutral_accounts_path) \\\n",
    "    + base_neutral_accounts\n",
    "    all_base_accounts = [re.sub(r'@','', acc) for acc in all_base_accounts]\n",
    "    \n",
    "    \n",
    "    # PATHS\n",
    "    all_accounts_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3'\n",
    "    \n",
    "    neutral_accounts_path = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\all_neutral_accounts'\n",
    "    anti_accounts_path_base = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\new_anti_hubs_'\n",
    "    pro_accounts_path_base = r'E:\\Twitter_data\\AUTOMATED_ITERATION_3\\new_pro_hubs_'\n",
    "        \n",
    "    for i in range(1, 20):\n",
    "        \n",
    "        # NEW ACCOUNTS \n",
    "        all_new_accounts = []\n",
    "        for acc_dir in os.listdir(all_accounts_path):\n",
    "            acc_dir_path = os.path.join(all_accounts_path, acc_dir)\n",
    "            accs = os.listdir(acc_dir_path)\n",
    "            all_new_accounts.extend(accs)\n",
    "        \n",
    "        all_accounts = all_base_accounts + all_new_accounts\n",
    "              \n",
    "        if i == 1:\n",
    "            \n",
    "            base_anti_hubs_followings_df = get_following_df_from_files(base_anti_accounts_path)\n",
    "            hand_picked_anti_hubs_followings_df = get_following_df_from_files(hand_picked_new_anti_hubs_path)\n",
    "\n",
    "            base_pro_hubs_followings_df = get_following_df_from_files(base_pro_accounts_path)\n",
    "            hand_picked_pro_hubs_followings_df = get_following_df_from_files(hand_picked_new_pro_hubs_path)\n",
    "            \n",
    "            all_followings = base_anti_hubs_followings_df + hand_picked_anti_hubs_followings_df + base_pro_hubs_followings_df \\\n",
    "            + hand_picked_pro_hubs_followings_df\n",
    "            # drop accounts which have more than 5000 followings\n",
    "            all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "            all_followings_df = pd.concat(all_followings, ignore_index=True)\n",
    "            \n",
    "            \n",
    "            # drop duplicates\n",
    "            all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "            # filter only for hubs\n",
    "            only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "            # clean description for language detection\n",
    "            only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "            only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "            # # take only en accounts\n",
    "            only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "            # # remove already downloaded accounts\n",
    "            only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts, all_acc=all_accounts)]\n",
    "            \n",
    "            new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()      \n",
    "            \n",
    "            anti_save_dir = f'{anti_accounts_path_base}{int(i)}'\n",
    "            pro_save_dir = f'{pro_accounts_path_base}{int(i)}'\n",
    "            \n",
    "            if not os.path.exists(anti_save_dir):\n",
    "                os.mkdir(anti_save_dir)\n",
    "            if not os.path.exists(pro_save_dir):\n",
    "                os.mkdir(pro_save_dir)\n",
    "            \n",
    "            get_tweets_for_all_accounts(api, anti_save_dir, pro_save_dir, neutral_accounts_path, new_hubs_accounts)\n",
    "            \n",
    "            new_anti_accounts = os.listdir(anti_save_dir)\n",
    "            new_pro_accounts = os.listdir(pro_save_dir)\n",
    "               \n",
    "            get_data(api, anti_save_dir, new_anti_accounts)\n",
    "            get_data(api, pro_save_dir, new_pro_accounts)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            anti_save_dir = f'{anti_accounts_path_base}{int(i-1)}'\n",
    "            pro_save_dir = f'{pro_accounts_path_base}{int(i-1)}'\n",
    "            \n",
    "            anti_hubs_followings_df = get_following_df_from_files(anti_save_dir)\n",
    "            pro_hubs_followings_df = get_following_df_from_files(pro_save_dir)\n",
    "            \n",
    "            all_followings = anti_hubs_followings_df + pro_hubs_followings_df\n",
    "            # drop accounts which have more than 5000 followings\n",
    "            all_followings = [acc for acc in all_followings if acc.shape[0] < 5000]\n",
    "            all_followings_df = pd.concat(all_followings, ignore_index=True)\n",
    "            \n",
    "            # drop duplicates\n",
    "            all_followings_df = all_followings_df.drop_duplicates(subset = [\"Screen_Name\"])\n",
    "            # filter only for hubs\n",
    "            only_hubs_df = all_followings_df[(all_followings_df['Follower_Count'] > 100000) & (all_followings_df['Follower_Count'] < 10000000) & (all_followings_df['StatusesCount'] > 2000)]\n",
    "            # clean description for language detectio\n",
    "            only_hubs_df['Description'] = only_hubs_df['Description'].apply(clean_text)\n",
    "            only_hubs_df['Description'] = only_hubs_df.Description.str.replace('[^a-zA-Z0-9]', ' ')\n",
    "            # # take only en accounts\n",
    "            only_hubs_df = only_hubs_df[only_hubs_df['Description'].apply(filter_en_accounts)]\n",
    "            # # remove already downloaded accounts\n",
    "            only_hubs_df = only_hubs_df[only_hubs_df['Screen_Name'].apply(filter_already_downloaded_accounts, all_acc=all_accounts)]\n",
    "            \n",
    "            new_hubs_accounts = only_hubs_df['Screen_Name'].tolist()\n",
    "            \n",
    "            anti_save_dir = f'{anti_accounts_path_base}{int(i)}'\n",
    "            pro_save_dir = f'{pro_accounts_path_base}{int(i)}'\n",
    "            \n",
    "            if not os.path.exists(anti_save_dir):\n",
    "                os.mkdir(anti_save_dir)\n",
    "            if not os.path.exists(pro_save_dir):\n",
    "                os.mkdir(pro_save_dir)\n",
    "            \n",
    "            get_tweets_for_all_accounts(api, anti_save_dir, pro_save_dir, neutral_accounts_path, new_hubs_accounts)\n",
    "            \n",
    "            new_anti_accounts = os.listdir(anti_save_dir)\n",
    "            new_pro_accounts = os.listdir(pro_save_dir)\n",
    "\n",
    "            \n",
    "            # end of iterations \n",
    "            if len(new_anti_accounts) == 0 and len(new_pro_accounts) == 0:\n",
    "                break\n",
    "            \n",
    "            get_data(api, anti_save_dir, new_anti_accounts)\n",
    "            get_data(api, pro_save_dir, new_pro_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35118f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_full_data_iteration()\n",
    "print('end of processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522bb95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
